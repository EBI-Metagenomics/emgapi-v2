import csv
from pathlib import Path
from typing import List

from prefect import task, get_run_logger

from analyses.models import Analysis
from workflows.models import (
    AssemblyAnalysisBatchAnalysis,
    AssemblyAnalysisPipelineStatus,
)
from workflows.flows.analyse_study_tasks.shared.analysis_states import AnalysisStates


@task
def set_post_assembly_analysis_states(
    assembly_current_outdir: Path, assembly_analyses_ids: List[int]
):
    """
    This function processes the end-of-execution reports generated by the assembly analysis pipeline
    to determine the outcome of each assembly and updates their status accordingly.

    TODO: this method is doing way too much, it's hard to follow and it has loads of side-effects that affect the
          flow of data. I think we need more atomic operations scatter in each pipeline batch.

    The pipeline produces two CSV files:
    - qc_failed_assemblies.csv: Lists assemblies that failed QC checks (assemblyID, reason)
    - analysed_assemblies.csv: Lists assemblies that completed successfully (assemblyID, info)

    Logic flow:
    1. Read QC failures from qc_failed_assemblies.csv if it exists
    2. Read successful analyses from analysed_assemblies.csv (raises error if the file is missing)
    3. For each assembly analysis:
    - If assembly is in QC failures: Mark workflow_status as FAILED and set status to ANALYSIS_QC_FAILED
    - If assembly is in successful analyses: Mark workflow_status as COMPLETED
    - If assembly is in neither list: Mark workflow_status as FAILED with unknown reason
    4. Bulk update all analyses to minimize database operations

    :param assembly_current_outdir: Path to the directory containing the pipeline output
    :param assembly_analyses_ids: List of assembly analyses IDs to process
    :raises ValueError: If the analysed_assemblies.csv file is missing (catastrophic failure)
    """
    # The pipeline produces top-level end-of-execution reports, which contain
    # the list of the assemblies that were completed and those that were not.
    # Similar to the amplicon pipeline

    logger = get_run_logger()

    # qc_failed_assemblies.csv: assemblyID,reason
    qc_failed_csv = Path(f"{assembly_current_outdir}/qc_failed_assemblies.csv")
    qc_failed_assemblies = {}  # Stores {assembly_accession, qc_fail_reason}

    if qc_failed_csv.is_file():
        logger.info("Reading qc failed assemblies...")
        with qc_failed_csv.open(mode="r") as file_handle:
            for row in csv.reader(file_handle, delimiter=","):
                assembly_accession, fail_reason = row
                qc_failed_assemblies[assembly_accession] = fail_reason

    # analysed_assemblies.csv: assemblyID, info
    analysed_assemblies_csv = Path(f"{assembly_current_outdir}/analysed_assemblies.csv")
    analysed_assemblies = {}  # Stores {assembly_accession, info}

    if analysed_assemblies_csv.is_file():
        with analysed_assemblies_csv.open(mode="r") as file_handle:
            for row in csv.reader(file_handle, delimiter=","):
                assembly_accession, info = row
                analysed_assemblies[assembly_accession] = info
    else:
        # The caller is responsible for handling this error -- which is quite catastrophic.
        raise ValueError(
            f"The end of run execution CSV file is missing. Expected path: {analysed_assemblies_csv}"
        )

    batch_relations = AssemblyAnalysisBatchAnalysis.objects.filter(
        analysis_id__in=assembly_analyses_ids
    ).select_related("analysis__assembly")

    relations_to_update = []
    analyses_to_update = []

    for batch_relation in batch_relations:
        analysis = batch_relation.analysis
        assembly_accession = analysis.assembly.first_accession

        if assembly_accession in qc_failed_assemblies:
            logger.error(f"QC failed - {analysis}")
            batch_relation.asa_status = AssemblyAnalysisPipelineStatus.FAILED
            relations_to_update.append(batch_relation)
            analysis.mark_status(
                AnalysisStates.ANALYSIS_QC_FAILED,
                set_status_as=True,
                reason=qc_failed_assemblies[assembly_accession],
                save=False,
            )
            analyses_to_update.append(analysis)
        elif assembly_accession in analysed_assemblies:
            logger.info(f"{analysis} marked as analyzed in the end of run CSV.")
            batch_relation.asa_status = AssemblyAnalysisPipelineStatus.COMPLETED
            relations_to_update.append(batch_relation)
        else:
            logger.error(f"Assembly {analysis} missing from CSV.")
            batch_relation.asa_status = AssemblyAnalysisPipelineStatus.FAILED
            relations_to_update.append(batch_relation)
            analysis.mark_status(
                AnalysisStates.ANALYSIS_QC_FAILED,
                set_status_as=True,
                save=False,
                reason=f"Assembly missing from {analysed_assemblies_csv}.",
            )
            analyses_to_update.append(analysis)

    # Bulk update - slightly gentler on the DB
    AssemblyAnalysisBatchAnalysis.objects.bulk_update(
        relations_to_update, ["asa_status"]
    )
    Analysis.objects.bulk_update(analyses_to_update, ["status"])
