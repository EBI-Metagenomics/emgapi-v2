name: emgapi
prefect-version: 2.18.3
build: null
push: null
pull:
- prefect.deployments.steps.set_working_directory:
    directory: /nfs/production/rdf/metagenomics/jenkins-slurm/dev-prefect-agent/
deployments:
- name: assemble_study_deployment
  version: null
  tags: []
  description: |-
    Get a study from ENA, and input it to MGnify.
    Kick off assembly pipeline.
    :param accession: Study accession e.g. PRJxxxxxx
    :param miassembler_profile: Name of the nextflow profile to use for MI Assembler.
  entrypoint: workflows/flows/assemble_study.py:assemble_study
  parameters: {}
  work_pool:
    name: slurm
    work_queue_name: null
    job_variables: {}
  schedules: []

- name: analysis_amplicon_study_deployment
  description: |-
    Get a study from ENA, and input it to MGnify.
    Kick off amplicon-v6 pipeline.
    :param study_accession: Study accession e.g. PRJxxxxxx
  entrypoint: workflows/flows/analysis_amplicon_study.py:analysis_amplicon_study
  work_pool:
    name: slurm

- name: analysis_assembly_study_deployment
  description: |-
    Get a study from ENA (or MGnify), and run assembly analysis the assemblies of the study.

    This then chains the Assembly Analysis Pipeline, VIRIfy and the Mobilome Annotation Pipeline using
    batches of assemblies to analise.

    It keeps track of the batches and their statuses.

    :param study_accession: e.g. PRJ or ERP accession
    :param workspace_dir: Path for the workspace dir. Defaults to the configured SLURM default workdir.
  entrypoint: workflows/flows/analysis/assembly/flows/analysis_assembly_study.py:analysis_assembly_study
  schedule: null
  work_pool:
    name: slurm

- name: run_assembly_analysis_pipeline_batch_deployment
  description: |-
    Run the assembly analysis pipeline-v6 for a given batch, including ASA, VIRify, and MAP.

    This function orchestrates the execution of the ASA pipeline, VIRify, and MAP. It handles tasks such as generating
    samplesheets, managing pipeline states, and storing relevant metadata into the database for further processing.

    Additionally, it imports analysis download files for each pipeline, validating results, and generates
    the study summary files for the batch.

    The flow is idempotent and can be restarted from any point:
    - ASA: Only processes analyses without asa_status=COMPLETED
    - VIRify: Only processes analyses with asa_status=COMPLETED and virify_status!=COMPLETED
    - MAP: Only processes analyses with virify_status=COMPLETED and map_status!=COMPLETED

    :param assembly_analyses_batch_id: Unique identifier for the assembly analysis batch to process.
    :type assembly_analyses_batch_id: uuid.UUID
    :raises ClusterJobFailedException: If the cluster job fails during the ASA pipeline execution.
    :raises Exception: For any unexpected errors encountered during the pipeline execution.
  entrypoint: workflows/flows/analysis/assembly/flows/run_assembly_analysis_pipeline_batch.py:run_assembly_analysis_pipeline_batch
  schedule: null
  work_pool:
    name: slurm

- name: run_virify_batch_deployment
  description: |-
    Runs the VIRify pipeline for a batch of assemblies.

    It expects the analyses to have been completed by ASA at this point. It will select those
    analyses and uses the ASA end-of-run virify samplesheet to pick up the assemblies to execute.

    The flow is idempotent: analyses that are already COMPLETED for VIRify will be skipped.
    Only analyses with asa_status=COMPLETED and virify_status!=COMPLETED will be processed.

    It doesn't validate the results or import them, but the results will be stored in the batch workspace.

    :param assembly_analyses_batch_id: Unique identifier for the assembly batch to be
        processed by the VIRify pipeline.
    :type assembly_analyses_batch_id: uuid.UUID
    :raises ClusterJobFailedException: If the VIRify pipeline execution job fails.
    :raises Exception: For any unexpected errors during pipeline execution.
  entrypoint: workflows/flows/analysis/assembly/flows/run_virify_batch.py:run_virify_batch
  schedule: null
  work_pool:
    name: slurm

- name: run_map_batch_deployment
  description: |-
    Run the MAP (Mobilome Annotation Pipeline) for a batch of assemblies.

    It expects the analyses to have been completed with ASA and VIRify at this point. It will select those
    analyses and generate a samplesheet for the MAP pipeline.

    The flow is idempotent: analyses that are already COMPLETED for MAP will be skipped.
    Only analyses with virify_status=COMPLETED and map_status!=COMPLETED will be processed.

    It doesn't validate the results or import them, but the results will be stored in the batch workspace.

    This flow won't raise an exception if the MAP pipeline fails unless raise_on_failure is set to True.
    This behavior is useful for batch processing, where we want to continue processing batches even if one fails.

    :param assembly_analyses_batch_id: The AssemblyAnalysisBatch to process
    :param raise_on_failure: Raise an exception if the MAP pipeline fails
  entrypoint: workflows/flows/analysis/assembly/flows/run_map_batch.py:run_map_batch
  schedule: null
  work_pool:
    name: slurm

- name: import_asa_batch_deployment
  description: |-
    Imports Assembly Analysis Pipeline results for a given batch.

    This flow will validate the results, update statuses based on analysis outcomes,
    and import data if validation succeeds.

    :param assembly_analyses_batch_id: The AssemblyAnalysisBatch to process
  entrypoint: workflows/flows/analysis/assembly/flows/import_asa_batch.py:import_asa_batch
  schedule: null
  work_pool:
    name: slurm

- name: import_virify_batch_deployment
  description: |-
    Imports VIRify analysis results for a given batch.

    :param assembly_analyses_batch_id: The AssemblyAnalysisBatch to process
  entrypoint: workflows/flows/analysis/assembly/flows/import_virify_batch.py:import_virify_batch
  schedule: null
  work_pool:
    name: slurm

- name: import_map_batch_deployment
  description: |-
    Imports MAP analysis results for a given batch.

    :param assembly_analyses_batch_id: The AssemblyAnalysisBatch to process
  entrypoint: workflows/flows/analysis/assembly/flows/import_map_batch.py:import_map_batch
  schedule: null
  work_pool:
    name: slurm

- name: finalize_assembly_study_deployment
  description: |-
    Run the last study level steps on study assembly analysis batches.
    This method checks that all the study batches have finished running, as in each
    batch 3 flows (aca, virify and map) are not running anymore. This is not ideal, and I'm
    working to improve it.
    TODO: Improve as is_running() is not enough, flows may have failed or haven't even started!
    TODO: Add unit tests
    TODO: Use events or automations to improve this

    This includes:
    - Merging assembly study summaries
    - Adding summaries to downloads
    - Copying v6 study summaries
    - Updating study features

    :param study_accession: Study accession (e.g., MGYS00001234)
    :type study_accession: str
  entrypoint: workflows/flows/analysis/assembly/flows/finalize_assembly_study.py:finalize_assembly_study
  work_pool:
    name: slurm

- name: nextflow_trace_etl_flow
  description: |-
    Nextflow Trace extraction and transformation flow.

    This flow orchestrates the extraction and transformation of Nextflow trace data
    from OrchestratedClusterJob models.

    :param sqlite_db_path: Path to the SQLite database where the transformed data will be stored
    :param batch_size: Number of database records to process at once
    :param min_created_at: Only process jobs created after this datetime
    :param max_created_at: Only process jobs created before this datetime
    :param only_completed: Only process completed jobs
    :param exclude_failed: Exclude failed jobs
  entrypoint: workflows/flows/nf_traces/flows.py:nextflow_trace_etl_flow
  work_pool:
    name: slurm

- name: analysis_rawreads_study_deployment
  description: |-
    Get a study from ENA, and input it to MGnify.
    Kick off raw-reads-v6 pipeline.
    :param study_accession: Study accession e.g. PRJxxxxxx
  entrypoint: workflows/flows/analysis_rawreads_study.py:analysis_rawreads_study
  work_pool:
    name: slurm

- name: import_genomes_flow_deployment
  description: |-
    Imports genomes from a catalogue directory into the database.

    This flow processes genome results from a catalogue directory, performs sanity checks,
    and imports genome data including annotations, files, and metadata.

    :param results_directory: Path to the catalogue directory containing genome results
    :param catalogue_name: Name of the genome catalogue
    :param catalogue_version: Version of the genome catalogue
    :param gold_biome: Biome classification for the catalogue
    :param pipeline_version: Version of the pipeline used to generate the genomes
    :param catalogue_type: Type of catalogue (e.g., prokaryotes, eukaryotes)
    :param catalogue_biome_label: Optional label for the catalogue biome
  entrypoint: workflows/flows/import_genomes_flow.py:import_genomes_flow
  work_pool:
    name: slurm

- name: import_genome_assembly_links_flow_deployment
  description: |-
    Imports data from a TSV file into the GenomeAssemblyLink model.

    This flow processes a TSV file containing genome assembly link information and
    imports it into the database, creating relationships between MAGs, genomes, and species representatives.

    :param tsv_path: Path to the TSV file containing genome assembly link data
  entrypoint: workflows/flows/import_genome_assembly_links_flow.py:import_genome_assembly_links_flow
  work_pool:
    name: slurm

- name: import_additional_contained_genomes_flow_deployment
  description: |-
    Imports data from a large TSV file into the AdditionalContainedGenomes model.

    The TSV must contain the following columns:
      - Run
      - Genome_Mgnify_accession
      - Containment
      - cANI

    The flow reads the file in streaming chunks and performs batched DB operations.

    :param csv_path: Path to the TSV file containing additional contained genomes data
    :param chunk_size: Size of chunks to read from the file (default: 50000)
    :param insert_batch_size: Size of batches for database insertion (default: 10000)
  entrypoint: workflows/flows/import_additional_contained_genomes_flow.py:import_additional_contained_genomes_flow
  work_pool:
    name: slurm

- name: update_ena_accession_from_json_flow_deployment
  description: |-
    Traverse per-genome JSON files to update Genome.ena_genome_accession from the
    'ncbi_genome_accession' value found in each file.

    :param base_dir: Directory containing one subdirectory per genome accession, each with
                     a JSON file named <accession>.json
    :param read_chunk_size: Django iterator chunk size when scanning genomes (default: 5000)
    :param update_batch_size: Number of rows to bulk update at once (default: 2000)
    :param catalogue_name: Optional; if provided, restrict processing to genomes whose
                         catalogue has this exact name
  entrypoint: workflows/flows/update_ena_accession_from_json_flow.py:update_ena_accession_from_json_flow
  work_pool:
    name: slurm

- name: upload_assembly_deployment
  description: |-
    This flow performs a sanity check and uploads an assembly for a specific run to ENA.

    It is intended to be executed *per run* after the assembly flow. The assembly uploader
    is a separate python library to prepare the upload files. The assembly submission
    via `webin-cli` is launched as a SLURM cluster job.

    :param assembly_id: ID of the assembly to upload
    :param dry_run: If True, perform a dry run without actual upload (default: True)
    :param custom_upload_folder: Optional custom path for upload folder
  entrypoint: workflows/flows/upload_assembly.py:upload_assembly
  work_pool:
    name: slurm
