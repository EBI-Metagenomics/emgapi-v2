---
version: '3'

includes:
  ebi-wp-k8s-hl:
    taskfile: ./deployment/ebi-wp-k8s-hl/Taskfile.yaml
    dir: ./deployment/ebi-wp-k8s-hl

tasks:
  prefect:
    desc: "Run a command on the prefect CLI. E.g. task prefect -- deployment"
    cmds:
      - docker compose run app prefectcli {{.CLI_ARGS}}

  create-pool:
    desc: "Create a Prefect workpool. E.g. NAME=slurm task create-process-pool"
    cmds:
      - docker compose exec app python manage.py prefectcli work-pool create --annotation_type {{.TYPE}} {{.CLI_ARGS}} {{.NAME}}
    vars:
      TYPE: "process"
      NAME: "slurm"

  deploy-flow:
    desc: "Build and deploy a prefect flow. E.g. FLOW=mgnify_run_pipeline_flow task deploy-flow"
    cmds:
      - docker compose --profile prefect up -d
      - docker compose run app prefectcli deploy {{ .FILE | default (list "workflows/flows/" .FLOW ".py" | join "") }}:{{.FLOW}} --name {{.FLOW}}_deployment -p {{.POOL}} --prefect-file workflows/prefect_deployments/prefect-dev-donco.yaml
    vars:
      POOL: "slurm"
    requires:
      vars: [FLOW]

  run:
    desc: "Run everything"
    cmds:
      - docker compose --profile all up {{.CLI_ARGS}}

  run-api-only:
    desc: "Run just the API (no slurm)"
    cmds:
      - docker compose --profile app up {{.CLI_ARGS}}

  stop:
    desc: "Stop everything"
    cmds:
      - docker compose --profile all down

  manage:
    desc: "Run a management command. E.g. task manage -- migrate"
    cmds:
      - docker compose run app {{.CLI_ARGS}}

  sbatch:
    desc: "Dispatch a slurm job to the mini slurm cluster. E.g. task sbatch -- --wait -t 0:00:30 --mem=10M --wrap=\"echo 'hello world'\" -o hello-world.out"
    cmds:
      - docker exec slurm_node "sbatch {{.CLI_ARGS}}"

  slurm:
    desc: "Connect to the slurm node to run commands there."
    cmds:
      - docker exec -it slurm_node bash
    interactive: true

  agent:
    desc: "Connect to the prefect agent to run commands there."
    cmds:
      - docker exec -it prefect-agent bash
    interactive: true

  test:
    desc: "Run pytest tests. E.g. `task test` or `task test -- -k study`"
    cmds:
      - docker compose run --entrypoint /bin/bash app -c "pytest {{.CLI_ARGS}}"

  testk:
    desc: "Run pytest for a subset of unit tests, without coverage noise. E.g. `task testk -- amplicon`"
    cmds:
      - docker compose run --entrypoint /bin/bash app -c "pytest --no-cov -s -k {{.CLI_ARGS}}"

  make-dev-data:
    desc: "Populate the app database with some demo data for development purposes"
    cmds:
      - docker compose --profile all down
      - docker volume rm -f emgapiv2_appdb  # force so that no error if volume doesn't exist.
      - docker compose run --entrypoint /bin/bash app -c "pytest -m dev_data_maker --no-cov -s"  # dumps a json to dev-db.json
      - docker compose run app migrate
      - docker compose run app loaddata dev-db.json
      - docker compose run app createsuperuser --username emgdev --email emg@example.org
      - rm -fr slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398
      - mkdir -p slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/
      - cp -r slurm-dev-environment/fs/nfs/public/tests/amplicon_v6_output/SRR1111111 slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/amplicon
      - docker compose run app shell -c "from analyses.models import Analysis; a=Analysis.objects.get(accession='MGYA00000004'); a.external_results_dir='PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/amplicon'; a.save()"

  debug-break-prefect-connection:
    desc: "Temporarily return 404s from the prefect server, when prefect worker tries to access it"
    vars:
      DURATION: 60
    cmds:
      - docker exec -d prefect-agent touch /tmp/prefect_is_404
      - echo "Interceptor active! Prefect API calls will now return 404."
      - sleep {{.DURATION}}
      - docker exec prefect-agent rm /tmp/prefect_is_404
    silent: false
    summary: |
      To test connection interruption, e.g.:
      $  FLOW=slow_cluster_job_example task deploy-flow
      $  task prefect -- deployment run  'Slow running cluster job example/slow_cluster_job_example_deployment' -p run_for_seconds=120
      ... wait for that flow to start its cluster job in prefect UI ...
      $ task debug-break-prefect-connection
      ... you will see that the worker stops reporting logs for a while, but should not crash thanks to retries ...

  debug-make-prefect-flow-with-zombie-job:
    desc: "Make a prefect flow in which there is a zombie cluster job (one where prefect worker died whilst running slurm job)"
    cmds:
      - task: prefect
        vars:
          {CLI_ARGS: "deployment run 'Slow running cluster job example/slow_cluster_job_example_deployment' -p run_for_seconds=60 -p slurm_resubmit_policy_minutes_ago=5"}
      - sleep 30
      - docker compose kill prefect-agent
      - sleep 60
      - docker compose start prefect-agent
    summary: |
      To make jobs in zombie state, for developing cleanup tools and investigating state management etc.
      You need to have deployed the `slow_cluster_job_example` flow to use this one, e.g.
      $ FLOW=slow_cluster_job_example task deploy-flow
      The task sets off a flow, that runs a 60second cluster job.
      ~Half way through that job, the prefect worker is killed for 60 seconds,
      so the cluster job should have finished whilst the worker was offline.
      This results in a zombie state flow and subflow, because the prefect server assumes the worker will
      somehow pick up where it left off.

      Can then use e.g.
      $ task manage -- reconnect_zombie_jobs -t 30
      to try and cleanup/retry the zombie flow.

      Since slurm_resubmit_policy_minutes_ago=5,
      if you run the reconnect_zombie_jobs shortly after the zombie job is made,
      you can verify that the slurm job actually continued to run or completed in the background and is
      "reconnected" to by the restarted flow.
      This is desired behaviour.
