---
version: "3.9"
services:
  slurm_db:
    image: mariadb:latest
    hostname: slurm_db
    container_name: slurm_db
    environment:
      MYSQL_RANDOM_ROOT_PASSWORD: "yes"
      MYSQL_DATABASE: slurm
      MYSQL_USER: slurm
      MYSQL_PASSWORD: slurm
    volumes:
      - db:/var/lib/mysql
    ports:
      - "3306:3306"
    profiles: ["slurm", "all", "slurm_full"]
    healthcheck:
      test: ['CMD', '/usr/local/bin/healthcheck.sh', '--connect']
      interval: 2s
      timeout: 2s
      retries: 5
    networks: [slurm]

  slurm_node:
    # A single node instance of slurm (dbd, controller, worker)
    build:
      context: .
      target: single_node
    command: ["slurmd"]
    hostname: slurm_node
    container_name: slurm_node
    depends_on:
      slurm_db:
        condition: service_healthy
    volumes:
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
      - "..:/app"
      - "./configs/slurm_prolog.sh:/usr/local/bin/slurm_prolog.sh"
      - "./configs/slurm_epilog.sh:/usr/local/bin/slurm_epilog.sh"
      - "./entrypoints/single-node-entrypoint.sh:/usr/local/bin/worker-entrypoint.sh"
      - "./fs/nfs/production:/nfs/production"
      - "./fs/hps:/hps"
      - "./fs/nfs/public:/publicnfs"
      - "./fs/nfs/ftp:/publicftp"
    expose:
      - "6818"
    profiles: ["slurm", "all"]
    networks: [slurm]

  slurm_db_daemon:
    build:
      context: .
      target: dbd
    command: ["slurmdbd"]
    hostname: slurm_db_daemon
    container_name: slurm_db_daemon
    volumes:
      - munge:/etc/munge
      - logs:/var/log/slurm
    expose:
      - "6819"
    depends_on:
      slurm_db:
        condition: service_healthy
    profiles: ["slurm_full"]
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "6819"]
      interval: 2s
      timeout: 2s
      retries: 10
    networks: [slurm]

  slurm_controller:
    hostname: slurm_controller
    container_name: slurm_controller
    build:
      context: .
      target: ctl
    command: ["slurmctld"]
    volumes:
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
    expose:
      - "6817"
    depends_on:
      slurm_db_daemon:
        condition: service_healthy
    profiles: ["slurm_full"]
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "6817"]
      interval: 30s
      start_interval: 2s
      timeout: 2s
      retries: 10
    networks: [slurm]

  slurm_worker:
    hostname: slurm_worker
    container_name: slurm_worker
    build:
      context: .
      target: worker
    command: ["slurmd"]
    volumes:
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
      - "..:/app"
      - "./fs/nfs/production:/nfs/production"
      - "./fs/hps:/hps"
      - "./fs/nfs/public:/publicnfs"
      - "./fs/nfs/ftp:/publicftp"
    expose:
      - "6818"
    depends_on:
      slurm_controller:
        condition: service_started
    profiles: ["slurm_full"]
    networks: [slurm]

volumes:
  db:
  munge:
  logs:
  jobdir:

networks:
  slurm:
    name: slurm