---
services:
  prefect-database:
    # Postgres for prefect server
    image: postgres:16
    restart: always
    container_name: prefect-database
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=prefect
    expose:
      - 5432
    ports:
      - "5433:5432"
    volumes:
      - prefectdb:/var/lib/postgresql/data
    profiles: ["prefect", "all"]
    networks: [emg]

  app-database:
    # Postgres for django app
    image: postgres:17
    restart: always
    container_name: app-database
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=emg
    expose:
      - 5432
    ports:
      - "5434:5432"
    volumes:
      - appdb:/var/lib/postgresql/data
    profiles: ["app", "prefect", "all"]
    networks: [emg]

  prefect-server:
    # Continuously running prefect server
    image: prefecthq/prefect:3.4.4-python3.12
    restart: always
    container_name: prefect-server
    volumes:
      - ./prefectserver:/root/.prefect
    entrypoint: ["/opt/prefect/entrypoint.sh", "prefect", "server", "start"]
    environment:
      - PREFECT_UI_URL=http://127.0.0.1:4200/api
      - PREFECT_API_URL=http://127.0.0.1:4200/api
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:postgres@prefect-database:5432/prefect
      - EXTRA_PIP_PACKAGES=httpx[cli] prefect-slurm==0.1.0
      - PREFECT_LOCAL_STORAGE_PATH=/root/.prefect/storage
    healthcheck:
      interval: 2s
      retries: 60
      start_period: 2s
      test: 'httpx $${PREFECT_API_URL}/ready | grep "\"message\": \"OK\"" || exit 1'
      timeout: 2s
    expose:
      - 4200
    ports:
      - "4200:4200"
    depends_on:
      prefect-database:
        condition: service_started
    profiles: ["prefect", "all"]
    networks: [emg]

  prefect-agent:
    # A prefect agent deployed in a separate node to the app
    build:
      context: .
      target: agent
    restart: always
    container_name: prefect-agent
    hostname: prefect_agent
    command: "prefectcli worker start --pool \"slurm\""
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - PREFECT_LOCAL_STORAGE_PATH=/root/.prefect/storage
      - DATABASE_URL=postgres://postgres:postgres@app-database:5432/emg
      - HTTP_PROXY=http://localhost:8080
      - http_proxy=http://localhost:8080
      - DJANGO_DEBUG=1
    profiles: ["prefect", "all"]
    env_file:
      - path: ./secrets-local.env
        required: false
    depends_on:
      prefect-server:
        condition: service_healthy
      app-database:
        condition: service_started
    networks: [emg, slurm]
    volumes:
      - ".:/app"
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
      - ./prefectserver:/root/.prefect
      - ./slurm-dev-environment/entrypoints/submitter-entrypoint.sh:/usr/local/bin/submitter-entrypoint.sh
      - "./slurm-dev-environment/fs/nfs/production:/nfs/production"
      - "./slurm-dev-environment/fs/nfs/ftp:/nfs/ftp"
      - "./slurm-dev-environment/fs/hps:/hps"

  prefect-slurm-worker:
    # A prefect-slurm worker deployed in a separate node to the app
    build:
      context: .
      target: agent
    restart: always
    container_name: prefect-slurm-worker
    hostname: prefect_slurm_worker
    command: prefect worker start --pool new-slurm --type slurm
    entrypoint: /usr/local/bin/prefect-slurm-entrypoint.sh
    environment:
      - PREFECT_SLURM_API_URL=http://slurm_node:6820
      - PREFECT_SLURM_USER_NAME=slurm
      - PREFECT_SLURM_TOKEN_FILE=/home/slurm/.prefect_slurm.jwt
      - PREFECT_API_URL=http://prefect-server:4200/api
      - DATABASE_URL=postgres://postgres:postgres@app-database:5432/emg
      - DJANGO_DEBUG=1
    profiles: ["prefect", "all"]
    env_file:
      - path: ./secrets-local.env
        required: false
    depends_on:
      prefect-server:
        condition: service_healthy
      app-database:
        condition: service_started
    networks: [emg, slurm]
    volumes:
      - ".:/app"
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
      - ./slurm-dev-environment/entrypoints/prefect-slurm-entrypoint.sh:/usr/local/bin/prefect-slurm-entrypoint.sh
      - ./slurm-dev-environment/fs/nfs/production:/nfs/production

  app:
    # Django app
    build:
      context: .
      target: django
    container_name: app
    profiles: ["app", "all"]
    command: "python manage.py runserver 0:8000"
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - PREFECT_UI_URL=http://localhost:4200
      - DATABASE_URL=postgres://postgres:postgres@app-database:5432/emg
      - EMG_WEBIN__JWT_SECRET_KEY=dummy-key
      - EMG_WEBIN__AUTH_ENDPOINT=http://httpbin/status/200
      - PRIVATE_DATA_SECURE_LINK_SECRET_KEY=dummy-key
      - DJANGO_DEBUG=1
    volumes:
      - ".:/app"
      - "./slurm-dev-environment/fs/nfs/public:/app/data"  # similar to the k8s volume mount in production
    depends_on:
      app-database:
        condition: service_started
        # prefect-server:
        # condition: service_healthy
      httpbin:
        condition: service_started
    ports:
      - "8000:8000"
    networks: [emg]

  transfer-services:
    # For mocking the "FTP" / transfer services area of EBI
    image: httpd:latest  # apache web server
    container_name: transfer-services
    ports:
      - "8080:80"
    volumes:
      - "./slurm-dev-environment/fs/nfs/ftp/public:/usr/local/apache2/htdocs/pub/"
      - "./slurm-dev-environment/configs/apache-override.conf:/usr/local/apache2/conf/httpd.conf:ro"
    restart: unless-stopped
    networks: [emg]
    profiles: ["all", "app"]

  private-data-server:
    # For serving private data with pre-signed links
    image: openresty/openresty:1.25.3.2-5-alpine-fat
    container_name: private-data-server
    ports:
      - "8081:80"
    volumes:
      - "./slurm-dev-environment/fs/nfs/public/services/private-data:/var/www/private-data/"
      - "./slurm-dev-environment/configs/private-data-nginx.conf:/usr/local/openresty/nginx/conf/nginx.conf"
    restart: unless-stopped
    networks: [emg]
    profiles: ["all", "app"]
    command: /bin/sh -c "opm get jkeys089/lua-resty-hmac && openresty -g 'daemon off;'"
    environment:
      - SECURE_LINK_SECRET=dummy-key

  httpbin:
    # For mocking APIs like ENA auth
    image: ghcr.io/psf/httpbin:0.10.2
    container_name: httpbin
    profiles: ["app", "all"]
    environment:
      - HTTPBIN_PORT=80
    ports:
      - "8088:80"
    networks: ["emg"]

  slurm_db:
    image: mariadb:latest
    hostname: slurm_db
    container_name: slurm_db
    environment:
      MYSQL_RANDOM_ROOT_PASSWORD: "yes"
      MYSQL_DATABASE: slurm
      MYSQL_USER: slurm
      MYSQL_PASSWORD: slurm
    volumes:
      - db:/var/lib/mysql
    ports:
      - "3306:3306"
    profiles: ["slurm", "all"]
    healthcheck:
      test: ['CMD', '/usr/local/bin/healthcheck.sh', '--connect']
      interval: 2s
      timeout: 2s
      retries: 5
    networks: [slurm]

  slurm_node:
    # A single node instance of slurm (dbd, controller, worker, restd)
    build:
      dockerfile: Dockerfile.slurm
      context: .
    command: ["slurmd"]
    hostname: slurm_node
    container_name: slurm_node
    depends_on:
      slurm_db:
        condition: service_healthy
    volumes:
      - munge:/etc/munge
      - logs:/var/log/slurm
      - jobdir:/opt/jobs
      - ".:/app"
      - "./slurm-dev-environment/configs/slurm_prolog.sh:/usr/local/bin/slurm_prolog.sh"
      - "./slurm-dev-environment/configs/slurm_epilog.sh:/usr/local/bin/slurm_epilog.sh"
      - "./slurm-dev-environment/entrypoints/entrypoint.sh:/usr/local/bin/entrypoint.sh"
      - "./slurm-dev-environment/fs/nfs/production:/nfs/production"
      - "./slurm-dev-environment/fs/hps:/hps"
      - "./slurm-dev-environment/fs/nfs/public:/publicnfs"
      - "./slurm-dev-environment/fs/nfs/ftp:/publicftp"
    expose:
      - "6818"
    ports:
      - "6820:6820"
    profiles: ["slurm", "all"]
    networks: [slurm, "emg"]

volumes:
  prefectdb:
  appdb:
  db:
  munge:
  logs:
  jobdir:

networks:
  emg:
    name: emg
  slurm:
    name: slurm
